A new & robust information theoretic measure and its application to image alignment. In this paper we develop a novel measure of information in a random variable based on its cumulative distribution that we dub cumulative residual entropy (CRE). This measure parallels the well known Shannon entropy but has the following advantages: (1) it is more general than the Shannon Entropy as its definition is valid in the discrete and continuous domains, (2) it possess more general mathematical properties and (3) it can be easily computed from sample data and these computations asymptotically converge to the true values. Based on CRE, we define the cross-CRE (CCRE) between two random variables, and apply it to solve the image alignment problem for parameterized (3D rigid and affine) transformations. The key strengths of the CCRE over using the mutual information (based on Shannon's entropy) are that the former has significantly larger tolerance to noise and a much larger convergence range over the field of parameterized transformations. We demonstrate these strengths via experiments on synthesized and real image data.