#!/bin/bash

LINE_ARGUMENTS=$#

if [[ $LINE_ARGUMENTS -eq 0 ]]; then
    export ABS_PATH=`pwd`

    JUDGECLASS="toyDataset"

else
    SKIP_DATA_EXTRACTION=$1; shift
    JUDGECLASS=$1; shift
    DEBUG_MODE=false; [[ $1 == "y" ]] && DEBUG_MODE=true; shift
    TOPICS_CONSIDERED=("$@");
    rules=5

fi




source "${ABS_PATH}/handle_errors"
source "${ABS_PATH}/colors"



PURPOSE=baseline

CORPLIST=("${JUDGECLASS}")
SOFIA="${ABS_PATH}/sofia-ml-read-only/sofia-ml"

$DEBUG_MODE && echo $JUDGECLASS

contains() {
    : '
        Utility function that returns true if a given
        value is within the given array and false otherwise.

        $1 -> value to check
        $2... -> array
    '


    value=$1; shift
    array=("$@")

    for v in "${array[@]}"; do
        if [[ $v == $value ]]; then
            true
            return
        fi
    done

    false
    return
}

: '
    Iterate over items in CORPLIST
'
for CORP in "${CORPLIST[@]}"; do


    echo -e "${BLUE}Working on topic $CORP...${END}"
    pushd Corpus # pushd <dir> é semelhante à cd <dir>


    echo -e "${BLUE}Preparando dataset...${END}"

    if [[ $LINE_ARGUMENTS -gt 0 ]]; then
        if [[ $SKIP_DATA_EXTRACTION == "false" ]]; then
            ./dofast "$CORP" $DEBUG_MODE
            cp "$CORP".df ../"$CORP".df > /dev/null

            cp "$CORP".svm.fil ../"$CORP".svm.fil
            echo -e "${GREEN}  runing feature compression SVD"

            ../svd/./script_create_svd.sh  ../$CORP.svm.fil 10

        fi

    fi






    popd # popd é semelhante ao cd ..


    : '
        Iterates over all `topic:query` pairs within
        judgement/$CORP.topic.stemming.txt
    '
    while IFS='' read -r line || [[ -n $line ]]; do

        IFS=':' read -ra TEXT <<< "$line"

        TOPIC="${TEXT[0]}"

        # Skips topics not considered
        if [[ $LINE_ARGUMENTS -gt 0 ]]; then
            if ! contains $TOPIC "${TOPICS_CONSIDERED[@]}"; then
                echo -e "${YELLOW}Skipping topic '$TOPIC'.${END}"
                continue
            fi
        fi

        QUERY="${TEXT[1]}"

        : '
            Checks if the TOPIC or QUERY are empty
        '
        if [[ -z $TOPIC ]]; then
            throw $EMPTY_VARIABLE_EXCEPTION "Variable TOPIC is empty"

        elif [[ -z $QUERY ]]; then
            throw $EMPTY_VARIABLE_EXCEPTION "Variable QUERY is empty"

        fi

        echo -e "${WHITE}Topic${END}:$TOPIC"
        echo -e "${WHITE}Query${END}:$QUERY"

        ######
        $DEBUG_MODE && echo "Creating directories to store results..."

        try
        (
            rm -rf result/"$PURPOSE"/"$CORP"/"$TOPIC"/
            mkdir -p result/"$PURPOSE"/"$CORP"/
            mkdir -p result/dump/"$PURPOSE"/"$CORP"/

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
        $DEBUG_MODE && echo -e "${BLUE}Creating topic directory topic (${TOPIC})...${END}"

        ######
        try
        (
            rm -rf $TOPIC
            mkdir $TOPIC
            #echo "cat judgement/qrels.$CORP.list | grep $TOPIC  | cut -d' ' -f3 > $TOPIC/goldendb"
            cat judgement/qrels.$CORP.list | grep "$TOPIC "  | cut -d' ' -f3 | sort > $TOPIC/goldendb

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
        $DEBUG_MODE && echo "Creating file 'N' with number of documents..."

        ######

        try
        (
            : '
                Creates a `N` file with number of all docs
            '
            echo `wc -l < "$CORP".svm.fil` > N

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
        $DEBUG_MODE && echo "Preparing 'docfils' file with all documents..."

        pushd $TOPIC

        echo "$QUERY" > "$TOPIC".seed.doc

        ######
        ln -n ../$CORP.svm.fil $CORP.svm.fil

        try
        (
            cut -d' ' -f1 ../$CORP.svm.fil | sed -e 's/.*/& &/' > docfil
            cut -d' ' -f1 docfil | cat -n > docfils

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }
        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
        $DEBUG_MODE && echo "Preparing relevance calculation files..."

        : '
            The `new$N` files keeps the files used on
                the $N-th iteration
        '

        ######
        try
        (
            touch rel.$TOPIC.fil
            touch prel.$TOPIC

            rm -rf prevalence.rate
            touch prevalence.rate

            rm -rf rel.rate
            touch rel.rate

            rm -f new[0-9][0-9].$TOPIC tail[0-9][0-9].$TOPIC self*.$TOPIC gold*.$TOPIC
            touch new00.$TOPIC

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }
        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"

        # Total number of documents
        NDOCS=`cat docfils | wc -l`
        # Number of documents already labeled
        NDUN=0
        # Number of documents to be labeled in the current iteration
        L=1

        R=100
        LAMBDA=0.0001
        NotRel=0
        flag_discretize=0
        flag_allac_first_time=0
        : '
            $TOPIC.seed.doc stores the $TOPIC query
        '

        cp $TOPIC.seed.doc ../$TOPIC.seed.doc

        popd


        $DEBUG_MODE && echo -e "${BLUE}Executing ./dofeaturesseed4...${END}"

        ./dofeaturesseed $TOPIC.seed.doc $TOPIC $CORP $DEBUG_MODE

        $DEBUG_MODE && echo -e "${GREEN}Finished ./dofeaturesseed4${END}"


        pushd $TOPIC

        ######
        try
        (
            sed -e 's/[^ ]*/0/' ../$CORP.svm.fil | ../dosplit
            echo  "Preparing ../$CORP.svm.fil (runs ../dosplit) "

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        try
        (
            sed -e 's/[^ ]*/1/' svm.$TOPIC.seed.doc.fil > $TOPIC.synthetic.seed
            echo  "Preparing $TOPIC.synthetic.seed..."

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }


        #key value database kissdb
            KEYSIZE=$(awk 'BEGIN{a=0}{len = length($1); a=a<len?len:a}END{print a}' $CORP.svm.fil)
            VALSIZE=$(awk 'BEGIN{a=0}{len = length($0); a=a<len?len:a}END{print a}' $CORP.svm.fil)
            KEYSIZE=$((KEYSIZE+2))
            VALSIZE=$((VALSIZE+2))

         try
        (





            if [ ! -f "../$CORP.db" ]; then

                $DEBUG_MODE && echo "Indexing $CORP, keysize = $KEYSIZE, valsize = $VALSIZE"

                ../indexer $CORP.svm.fil "$CORP".db $KEYSIZE $VALSIZE || (echo "Error creating db"; exit 1)
            fi


        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }





#             fi
#         fi
        #pushd $TOPIC

        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"


        for x in 0 1 2 3 4 5 6 7 8 9; do

            for y in 0 1 2 3 4 5 6 7 8 9 ; do

                if [ $NDUN -lt $NDOCS ] ; then
                    export N=$x$y

                    echo "Runing round number  $N with $((NDOCS-NDUN)) documents"
                    ######
                    $DEBUG_MODE && echo "Preparing trainset..."
                    
                    

                    try
                    (
                        cp $TOPIC.synthetic.seed trainset

                        cut -f2 docfils | shuf -n $R | sort | .././indexer "$CORP".db $KEYSIZE $VALSIZE | sed -e's/[^ ]*/-1/' >> trainset

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
                    $DEBUG_MODE && echo "Preparing seed (and x)... `wc -l < trainset`"

                    ######
                    try
                    (
                        cat sub_new[0-9][0-9].$TOPIC > seed
                        #cat ssarp* > seed
                        #cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' > x
                        #cat seed | sort | join -v1 - rel.$TOPIC.fil | sort -R | head -50000 | sed -e 's/^/-1 /' >> x

                        cat seed | sort | join - rel.$TOPIC.fil | sed -e 's/^/1 /' | sort | uniq > x
                        cat seed | sort | join -v1 - rel.$TOPIC.fil | shuf -n 50000 | sed -e 's/^/-1 /' | sort | uniq  >> x

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    $DEBUG_MODE && echo -e "${GREEN}Done. ${END} "
                    $DEBUG_MODE &&  echo "Preparing trainset (pt 2)... `wc -l < trainset`"

                    ######
                    try
                    (
                        #sort -k2 x | join -12 - ../$CORP.svm.fil | cut -d' ' -f2- | sort -n >> trainset
                        cut -d' ' -f2 x | .././indexer $CORP.db $KEYSIZE $VALSIZE | cut -d' ' -f2- | paste -d' ' <(cut -d' ' -f1 x) - | sort -n >> trainset

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    $DEBUG_MODE && echo -e "${GREEN}Done.${END}"

                    # Calculate relevant documents prevalence rate in the traning set
                    RELTRAINDOC=`grep -E "^1\b" trainset | wc -l`

                    NOTRELTRAINDOC=`grep -E "^-1\b" trainset | wc -l`

                    PREVALENCERATE=`echo "scale=4; $RELTRAINDOC / ($RELTRAINDOC + $NOTRELTRAINDOC)" | bc`

                    echo $RELTRAINDOC $NOTRELTRAINDOC $PREVALENCERATE >> prevalence.rate

                    ######
                    $DEBUG_MODE && echo "Training (Running SOFIA-ML)... `wc -l < trainset`"

                    try
                    (


                        $SOFIA --learner_type logreg-pegasos --loop_type roc --lambda $LAMBDA --iterations 2000000 --training_file trainset --dimensionality 3300000  --model_out svm_model &> saida_classificador


                        RES=$?

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }


                    $DEBUG_MODE && echo -e "${GREEN}Finished SOFIA-ML.${END}"
                    $DEBUG_MODE &&  echo $RES

                    MAXTHREADS=5
                    if [[ "$RES" -eq "0" ]] ; then
                        for z in svm.test.* ; do

                            ######


                            while [ "$(jobs | grep 'Running' | wc -l)" -ge "$MAXTHREADS" ]; do
                                sleep 1
                            done



                            try
                            (

                                $SOFIA --test_file $z --dimensionality 3300000 --model_in svm_model --results_file pout.$z &> saida_classificador &


                            ) 2> $STD_ERROR_OUT

                            catch || {
                                echo $ERROR_CODE
                                exit_on_error
                            }

                        done

                        wait
                        #waiting all threads complete their work

                    else
                        ######

                        try
                        (
                            rm -f pout.svm.test.*
                            cut -f2      | sort -R | cat -n | sort -k2 | sed -e 's/ */-/' > pout.svm.test.1

                        ) 2> $STD_ERROR_OUT


                        $DEBUG_MODE && echo -e "${GREEN}Done.${END}"

                    fi

                    ######
                    $DEBUG_MODE && echo "Starting SCAL process $N\nPreparing ranking.$N.$TOPIC..."


                    try
                    (
                        cat new[0-9][0-9].$TOPIC > seed.$TOPIC
                        cut -f1 pout.svm.test.* | ../fixnum | cat -n | join -o2.2,1.2 -t$'\t' - docfils | sort -k1 -n  > inlr.out.$N.$TOPIC

                        $DEBUG_MODE && echo -e "${BLUE}\tinlr.out size =`wc -l < inlr.out.$N.$TOPIC` docfils  `wc -l <docfils `${END}"

                        sort -n seed.$TOPIC > temp
                        cat temp | join  -v2 - inlr.out.$N.$TOPIC -2 1 | shuf |  sort -k 2 -r -g -s  > ranking.$N.$TOPIC
                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }

                    $DEBUG_MODE && echo -e "${GREEN}Done.${END}"
                    $DEBUG_MODE && echo "Preparing new$N.$TOPIC..."


                    ######
                    try
                    (
                        $DEBUG_MODE && echo -e "${BLUE}\tranking size `wc -l < ranking.$N.$TOPIC`${END}"

                        cat ranking.$N.$TOPIC | cut -d' ' -f1 > new$N.$TOPIC
                        cp new$N.$TOPIC U$N

                        cat new[0-9][0-9].$TOPIC > x

                        if [ "$N" != "99" ] ; then
                            head -$L new$N.$TOPIC > y
                            mv y new$N.$TOPIC
                        fi

                    ) 2> $STD_ERROR_OUT

                    catch || {
                        exit_on_error
                    }
                    $DEBUG_MODE && echo -e "${GREEN}Done.${END}"

                    : '
                        x armazena new($N - 1).$TOPIC
                    '

                    # Limits the number of documents by 30
                    if [ $L -le 30 ]; then
                        b=$L
                    else
                        b=30
                    fi

                    : '
                        The files sub_new$N represents a sample of at most 30
                            documents from the documents to be used in the iteration.
                    '
                    shuf -n $b new$N.$TOPIC > sub_new$N.$TOPIC

                    $DEBUG_MODE && echo -e "${BLUE}Number of labeled pairs: `wc -l < sub_new$N.$TOPIC`${END}"




                    # judgefile tells which are the positive docs
                    python2.7 ../doJudgementMain.py --topic=$TOPIC --judgefile=../judgement/qrels.$JUDGECLASS.list --input=sub_new$N.$TOPIC --output=rel.$TOPIC.Judged.doc.list --record=$TOPIC.record.list

                    # rel.$TOPIC.Judged.doc.list contains the current relevant docs

                    cat rel.$TOPIC.Judged.doc.list >> rel.$TOPIC.fil
                    cat rel.$TOPIC.Judged.doc.list > rel.$TOPIC.$N.Judged.doc.list
                    sort rel.$TOPIC.fil | uniq > temp

                    mv temp rel.$TOPIC.fil 
                 
                    
                 
                 
                
                        
                        




                    $DEBUG_MODE && echo -e "${GREEN}Starting active learning${END}"

                 ############active learning


                    if [ "$NotRel" -lt 3 ] || [ "$Rel" -lt 1000 ]
                    then

                        echo "store seed in ssarp file"

                        cp sub_new$N.$TOPIC ssarp$N.$TOPIC

                        cat sub_new$N.$TOPIC  | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > x_posit.$N

                        cat sub_new$N.$TOPIC   | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' > x_negat.$N




                        cat  x_negat.*   |  sort -k2  | join - ../$CORP.svm.fil.svd   -2 1 -1 2 > seed_ssarp.$TOPIC


                        cat  x_posit.* | shuf -n 1 | sort -k2  | join - ../$CORP.svm.fil.svd  -2 1 -1 2 >> seed_ssarp.$TOPIC


                        cut -d ' ' -f2- seed_ssarp.$TOPIC  > seed_ssarpB.$TOPIC
            #

                        cp x_posit.$N sub_new_positivo.$N
                        cp x_negat.$N sub_new_negativo.$N
                        ssarp_relevants=`wc -l < x_posit.$N`
                    else

                        echo "creating files"

                        cat sub_new$N.$TOPIC | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > temp_posit.$N.$TOPIC
                        cat sub_new$N.$TOPIC | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' > temp_negat.$N.$TOPIC


                        cat temp_posit.$N.$TOPIC temp_negat.$N.$TOPIC |  sort -k2  | join - ../$CORP.svm.fil.svd  -2 1 -1 2 > trainset.$N.$TOPIC
                        cut -d ' ' -f2- trainset.$N.$TOPIC  > trainsetB.$N.$TOPIC



                        if [ "$flag_discretize" -eq '0' ]
                        then
                            echo "gerando os bins\n"
                            echo "python3 ../svd/convert_txt.py ../$CORP.svm.fil.svd /tmp/total.$TOPIC.arff rel.$TOPIC.fil   "
                            python3 ../svd/convert_txt.py ../$CORP.svm.fil.svd /tmp/total.$TOPIC.arff rel.$TOPIC.fil

                            cd ../SSARP/run/
                            rm train*
                            .././gera_bins_TUBE.sh /tmp/total.$TOPIC.arff  50 10 10
                            cd -
                            echo "saindo da geração dos bins"
                            flag_discretize=1
                        fi

                        echo "convertendo arquivo para txt"
                            python3 ../svd/convert_txt.py trainsetB.$N.$TOPIC out.$N.$TOPIC.arff rel.$TOPIC.fil
                            python3 ../svd/convert_txt.py seed_ssarpB.$TOPIC seed_out.$N.$TOPIC.arff rel.$TOPIC.fil


                            cp trainset.$N.$TOPIC ../SSARP/run/
                            cp seed_out.$N.$TOPIC.arff ../SSARP/run/
                            cp out.$N.$TOPIC.arff ../SSARP/run/


                        echo "executando alac $rules"
                            cd ../SSARP/run/
                            ./SSARPX.sh out.$N.$TOPIC trainset.$N.$TOPIC 50 $N seed_out.$N.$TOPIC.arff $TOPIC $flag_allac_first_time $rules
                            flag_allac_first_time=$(($flag_allac_first_time+1))
                            cat label.$N.$TOPIC > /tmp/ssarp$N.$TOPIC
                            cd -
                            mv /tmp/ssarp$N.$TOPIC .

                            cat sub_new$N.$TOPIC  | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > sub_new_positivo.$N
                            cat sub_new$N.$TOPIC   | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' > sub_new_negativo.$N

                            cat ssarp$N.$TOPIC   | sort | uniq | join - rel.$TOPIC.fil | cut -d' ' -f1 | sed -e 's/^/1 /' > x_posit.$N
                            cat ssarp$N.$TOPIC   | sort | uniq | join - rel.$TOPIC.fil -v1 | cut -d' ' -f1 | sed -e 's/^/-1 /' > x_negat.$N

                            cat x_posit.$N x_negat.$N | cut -d' ' -f2 | sort | uniq > ssarp$N.$TOPIC
                            ssarp_relevants=`wc -l < x_posit.$N`

                            echo "docs positivos coletados `wc -l < x_posit.$N`   docs negativos ----  `wc -l < x_negat.$N` --- total docs positivos `wc -l < sub_new_positivo.$N`" >> log_run

                        fi




                    aux=`wc -l < x_negat.$N`

                    NotRel=$(($NotRel+$aux))               

    #
                    $DEBUG_MODE && echo "End of active learning step"
                    ###########################################



                    RELFINDDOC=`wc -l < x_posit.$N`
                    RELRATE=`echo "scale=4; $RELFINDDOC / $L" | bc`
                    CURRENTREL=`cat  x_posit.[0-9][0-9] | wc -l`

                    alreadyLabeledDocs=`cat sub_new[0-9][0-9].$TOPIC   | wc -l`
                    allDocs=`cat new[0-9][0-9].$TOPIC  | wc -l`

                    $DEBUG_MODE && echo "rel $RELFINDDOC L $L b $b  alreadyLabeledDocs $alreadyLabeledDocs  allDocs $allDocs REL $RELRATE CURRENTREL $CURRENTREL relevant docs `wc -l < rel.$TOPIC.fil` "

                    aux=$((($RELFINDDOC*$L)/$b))
                    Rel=$(($Rel+$aux*1000))

                    echo $RELFINDDOC $L $b $RELRATE $CURRENTREL >> rel.rate
                    sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC

                    cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

                    $DEBUG_MODE && echo "compute the Estimate ρ̂  of each round"

                    Temp_A=$(($RELFINDDOC*$L))
                    Temp_B=$(($Temp_A/$b))

                    export Estimate=$(($Estimate+$Temp_B))
                    echo "${N%.*} $Estimate" >> store_estimation

                    echo -e "${GREEN} $Temp_A $Temp_B $b $L Estimate rate is $Estimate"

                    NDUN=$((NDUN+L))
                    L=$((L+(L+9)/10))
                fi
            done
        done


        echo "select the top 95% relevant documents  from the last ranking"
        echo " 20: Estimate ρ̂ = 1.05 "
        export prevalence=`echo "scale=5; ($Estimate * 1.05) / $NDOCS  " | bc`
        echo "prevalence $prevalence"
        export m=`echo "scale=5; ($prevalence * $NDOCS ) * 0.90 " | bc`
        prevalence_int=${m%.*}

        python3 ../selectRound.py store_estimation $prevalence_int


        export j=`cat flagOut`

        export t=`wc -l < U$j`




        NumberDocument=$(($NDOCS-$t))
        sort -k 2 -n inlr.out.$N.$TOPIC > sorted_ranking
        tail -$NumberDocument sorted_ranking | cut -d$'\t' -f1 > result_ranking.$TOPIC

        n=$(($NDOCS-$prevalence_int))
        tail -$NDOCS sorted_ranking | cut -d' ' -f1 | head -$n > result_plus.$TOPIC

        cat result_ranking.$TOPIC > result 
        cat sub_new[0-9][0-9]* >> result
        cat result | sort | uniq > temp
        mv temp result        

        echo "valor t =$t valor j =$j valor NumberDocument =$NumberDocument NDOCS=$NDOCS"

        cat ssarp[0-9][0-9].$TOPIC | sort > alreadyLabeledDocs.$TOPIC
        cat sorted_ranking | cut -d' ' -f1   > ranking.$TOPIC
        awk 'BEGIN {FS=OFS=" "; Q=" "} {print Q (NR==0 ? ID : ++n) Q, $0}'  ranking.$TOPIC > ranking_with_order.$TOPIC
    
        sort  -k2 ranking_with_order.$TOPIC | join - alreadyLabeledDocs.$TOPIC  -12 -v1 | sort -n  -k2 | cut -d' ' -f1 > ranking_final.$TOPIC
        
        
      
        


        RelevantDocs=`cat result |  join - goldendb |  wc -l`
        NumberDocsResult=`wc -l < result`
        
        
        totalRelevants=`wc -l < goldendb`
        recall=`echo "scale=5; ($RelevantDocs / $totalRelevants)" | bc`
        precisao=`echo "scale=5; ($RelevantDocs / $NumberDocsResult)" | bc`
        echo "Precisão: $precisao" >> n_docs_ranking
        echo "Recall: $recall" >> n_docs_ranking
        echo "Tamanho ranking: $NumberDocument" >> n_docs_ranking
        echo "Numero de positivos encontrados: $RelevantDocs" >>n_docs_ranking
        echo "----------------------------------------------------------" 
        echo "RESULTADO FINAL:"
        echo "    Documentos Rotulados: $NumberDocsResult"
        echo "    Total de positivos existentes: $totalRelevants"
        echo "    Positivos encontrados: $RelevantDocs"
        echo "    Recall: $recall"
        echo "    Precisao: $precisao"
        echo "    Tamanho do Ranking Final: $NumberDocument" 
        echo "----------------------------------------------------------"    
        
         #rm -rf svm.test.*
        


       




        total=`wc -l < goldendb`
        recall=`echo "scale=5; ($relNumber / $total)" | bc`
        precisao=`echo "scale=5; ($relNumber / $NumberDocument)" | bc`
        echo "resultado final SCAL $TOPIC - $relNumber ------$NumberDocument  recall $recall  precisao $precisao"




        echo -e "${BLUE}calling second sampling strategy proposed by REVEAL ${END}"
        ../script_select_pairs.sh $TOPIC $JUDGECLASS $prevalence_int $rules 20

# 
 popd
          mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
          #rm $TOPIC.seed.doc
 popd
# 
# 
#          mv $TOPIC result/"$PURPOSE"/"$CORP"/$TOPIC
#          rm $TOPIC.seed.doc

    done < "judgement/$CORP.topic.stemming.txt"

   # echo -e "${WHITE}Apos finalizar o método o numero de docs recuperados do método  será o top total documentos acessados até a ultima executação que encontrou documentos relevantes. Por exemplo, se a execução N =20 foi a ultima a achar documentos releventas na amostragem deve-se pegar os top ~300 como relevantes. Essa será saida do método. Ou seja, o arquivo rel.rate armazena os relevantes recuperados, é só usar ele para implementar isso.${END}"


    #rm -rf "$CORP".svm.fil
    #rm "$CORP".df

    #rm N

    # Generate LSI from tfdf
    # python clustering/doLSI.py --input=tfdf_oldreut --output=LSIVector/"$CORP".lsi.dump --mapping=LSIVector/"$CORP".mapping.dump --latent=200 --choice=entropy --normalization=yes

done
