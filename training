#!/bin/bash

#===============================================================================
#       USAGE:  ./training <topic> <query> <judgeclass> <verbose>
#                          <keysize> <valsize>
#
# DESCRIPTION:  Contains the training loop that judges files as relevant or not.
#===============================================================================

source "${ABS_PATH}/handle_errors"
source "${ABS_PATH}/colors"

SOFIA="${ABS_PATH}/sofia-ml-read-only/sofia-ml"

TOPIC=$1; shift
QUERY=$1; shift
JUDGECLASS=$1; shift
VERBOSE=false; [[ $1 == "true" ]] && VERBOSE=true; shift
KEYSIZE=$1; shift
VALSIZE=$1; shift
ROUND=$1; shift

MAXTHREADS=5
rules=5

pushd $TOPIC


R=100                                   # Document sample size going to trainset
LAMBDA=0.0001                           # Lambda parameter for SVM
TOTAL_DOCUMENTS=`cat docfils | wc -l`   # Total quantity of documents
LABELED_DOCUMENTS=0                     # Total quantity of labeled documents
SAMPLE_SIZE=1                           # Number of documents to be labeled in the current round
TEMP_FILE=$(mktemp)                     # Temporary file for file redirections
sub_sample=0                            # Sub sample, limited to 30

MISSING_REL=0                           # Total number of relevant docs missed by active learning
discretize=1                            #if the dataset need to be discretized to run active learning
flag_allac_first_time=1
estimate=0                               #

not_relevants=`cat x_negat.* | wc -l `                          # Documents not relevant so far
relevants=`cat x_posit.* | wc -l `                              # Documents relevant so far

if [ $(($not_relevants+$relevants)) -gt 30 ]; then
	SAMPLE_SIZE=30
else 
	SAMPLE_SIZE=$(($not_relevants+$relevants))
fi

function check_file {
    if [ `wc -l < $1` -eq 0 ]; then
            $VERBOSE && e_secondary "Error FILE $1 EQUAL TO ZERO"
            break;
    fi
}

temp_round=200
memory=0
all_positive=0


#-------------------------------------------------------------------------
# performs the 100 looping rounds
#-------------------------------------------------------------------------
for _round in $(seq -f "%02g" $ROUND 99); do
    
  
    if [ $LABELED_DOCUMENTS -lt $TOTAL_DOCUMENTS ]; then
        round=$_round

        $VERBOSE && e_primary "Preparing trainset from round $round wiht $relevants docs relevant "

        try
        (
            #cp $TOPIC.synthetic.seed trainset

            cut -f2 docfils \
                | shuf -n $R \
                | sort \
                | .././indexer "$TOPIC".db $KEYSIZE $VALSIZE \
                | sed -e's/[^ ]*/-1/' >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing seed..."

        try
        (
            cat ssarp* > seed
    
            cat seed \
                | sort \
                | join - goldendb.$TOPIC \
                | sed -e 's/^/1 /' \
                | sort \
                | uniq > intermediate_seed

            cat seed \
                | sort \
                | join -v1 - goldendb.$TOPIC \
                | shuf -n 50000 \
                | sed -e 's/^/-1 /' \
                | sort \
                | uniq  >> intermediate_seed

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Adding content from seed to trainset..."

        try
        (
            cut -d' ' -f2 intermediate_seed \
                | .././indexer $TOPIC.db $KEYSIZE $VALSIZE \
                | cut -d' ' -f2- \
                | paste -d' ' <(cut -d' ' -f1 intermediate_seed) - \
                | sort -n >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # calculate the prevalence of relevant documents on training set
        #-----------------------------------------------------------------
        REL_ON_TRAINSET=`grep -E "^1\b" trainset | wc -l`
        NOT_REL_ON_TRAINSET=`grep -E "^-1\b" trainset | wc -l`
        PREVALENCE_RATE=`echo "scale=4;
            $REL_ON_TRAINSET / ($REL_ON_TRAINSET + $NOT_REL_ON_TRAINSET)" \
            | bc`

        echo $REL_ON_TRAINSET $NOT_REL_ON_TRAINSET $PREVALENCE_RATE >> prevalence.rate

        $VERBOSE && e_primary "Executing training algorithm (sofia-ml)..."
       
#         try
#         (
            $SOFIA --learner_type logreg-pegasos --loop_type roc \
                --lambda $LAMBDA --iterations 2000000 \
                --training_file trainset --dimensionality 3300000 \
                --model_out svm_model &> classifier_output  

            sofia_status=$?

#         ) 2> $STD_ERROR_OUT
# 
#         catch || {
#             exit_on_error
#         }
        check_file trainset
        
        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # tests the previously trained model
        #-----------------------------------------------------------------
        if [[ $sofia_status -eq "0" ]]; then
            for test_file in svm.test.*; do
                while [ "$(jobs | grep 'Running' | wc -l)" -ge "$MAXTHREADS" ]; do
                    sleep 1
                done

                $VERBOSE && e_primary "Testing previously trained model..."

#                  try
#                 (
                    $SOFIA --test_file $test_file --dimensionality 3300000 \
                        --model_in svm_model --results_file pout.$test_file  &>> classifier_output
                             

#                  ) 2> $STD_ERROR_OUT
#  
#                catch || {
#                      exit_on_error
#                  }

                $VERBOSE && e_success "Done."

            done

            wait

        #-----------------------------------------------------------------
        # create test files
        #-----------------------------------------------------------------
        else
            try
            (
                #rm -f pout.svm.test.*
                cut -f2 docfils \
                    | sort -R \
                    | cat -n \
                    | sort -k2 \
                    | sed -e 's/ */-/' > pout.svm.test.1

            ) 2> $STD_ERROR_OUT

            catch || {
                exit_on_error
            }
        fi
        check_file pout.svm.test.*
        $VERBOSE && e_primary "Preparing ranking... "

#         try
#         (
            cat new[0-9][0-9].$TOPIC  | sort | uniq >  LABELED_DOCS.$TOPIC
            echo "LABELED_DOCS.$TOPIC ---> `wc -l < LABELED_DOCS.$TOPIC`"
            cat x_posit.* x_negat.* | cut -d' ' -f2  >> LABELED_DOCS.$TOPIC
            sort  LABELED_DOCS.$TOPIC | uniq > temp
            mv temp LABELED_DOCS.$TOPIC
            
            cut -f1 pout.svm.test.* \
                | ../fixnum \
                | cat -n \
                | join -o2.2,1.2 -t$'\t' - docfils \
                | sort -k1   > inlr.out."$round".$TOPIC

                      
            check_file inlr.out."$round".$TOPIC
            
            sort  LABELED_DOCS.$TOPIC \
                | join  -v2 - inlr.out."$round".$TOPIC -2 1 \
                | shuf \
                | sort -k 2 -r -g -s  > ranking."$round".$TOPIC
            echo "ranking."$round".$TOPIC ---> `wc -l < ranking."$round".$TOPIC`"    
            check_file ranking."$round".$TOPIC

#         ) 2> $STD_ERROR_OUT
# 
#         catch || {
#             exit_on_error
#         }
        
        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing new"$round".$TOPIC."

        try
        (
            cat ranking."$round".$TOPIC \
                | cut -d' ' -f1 > new"$round".$TOPIC

            #cat new[0-9][0-9] > intermediate_seed

            if [ ""$round"" != "99" ] ; then
                head -$SAMPLE_SIZE new"$round".$TOPIC > $TEMP_FILE
                mv $TEMP_FILE new"$round".$TOPIC
            fi

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SCAL subsampling..."

        if [ $SAMPLE_SIZE -le 30 ]; then
            sub_sample=$SAMPLE_SIZE
        else
            sub_sample=$SAMPLE_SIZE
        fi

        head -n $sub_sample new"$round".$TOPIC  | sort -k2> sub_new"$round".$TOPIC
    
        temp=`cat new"$round".$TOPIC  | sort | join - goldendb.$TOPIC | wc -l `
        all_positive=$(($temp+$all_positive))
        # TODO: Check the viability of adding the output
        # of this script to a variable instead in a file

        python2.7 ../doJudgementMain.py --topic=$TOPIC \
            --judgefile=../judgement/qrels.$JUDGECLASS.list \
            --input=sub_new"$round".$TOPIC \
            --output=rel.$TOPIC."$round".Judged.doc.list \
            --record=$TOPIC.record.list

        cat rel.$TOPIC."$round".Judged.doc.list >> rel.$TOPIC.fil
        sort rel.$TOPIC.fil | uniq > $TEMP_FILE
        mv $TEMP_FILE rel.$TOPIC.fil

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SSARP stage..."

        #-----------------------------------------------------------------
        # the active learning method needs at least 1 negative and
        # 1 positive document
        #-----------------------------------------------------------------
        if [ $not_relevants -lt 1 ] || [ $relevants -lt 1 ]; then
            $VERBOSE && e_secondary "\tStoring seed in ssarp file..."

            cp sub_new"$round".$TOPIC ssarp"$round".$TOPIC

            cat sub_new"$round".$TOPIC \
                | sort \
                | uniq \
                | join - goldendb.$TOPIC \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > x_posit."$round"

            cat sub_new"$round".$TOPIC \
                | sort \
                | uniq \
                | join - goldendb.$TOPIC -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > x_negat."$round"

            cat  x_negat.* \
                | sort -k2 \
                | join - $TOPIC.svm.fil.svd -2 1 -1 2 > seed_ssarp_docs.$TOPIC

            cat  x_posit.* \
                | shuf -n 1 \
                | sort -k2 \
                | join - $TOPIC.svm.fil.svd -2 1 -1 2 >> seed_ssarp_docs.$TOPIC

            cut -d ' ' -f2- seed_ssarp_docs.$TOPIC  > seed_ssarp_labels.$TOPIC
            
            cp x_posit."$round" sub_new_positives."$round"
            cp x_negat."$round" sub_new_negatives."$round"
            cat  x_posit."$round" >> evalutionclef.$TOPIC.txt
            cp x_negat."$round"  >> evalutionclef.$TOPIC.txt
            ssarp_relevants=`wc -l < x_posit."$round"`

        #-----------------------------------------------------------------
        #
        #-----------------------------------------------------------------
        else
            $VERBOSE && e_primary "Call SSARP with input size `wc -l < sub_new"$round".$TOPIC` sub sub_new"$round".$TOPIC "
            
            source ../active_learning $CORPUS `echo $VERBOSE` $TOPIC "$round" $discretize 2  sub_new"$round".$TOPIC  2        
            discretize=1
            
            rel_found_sample=`wc -l < x_posit."$round"`  # relevants on previous sample
            i=0
            acc=$rel_found_sample
            roundSize=30
#             while [ $rel_found_sample -gt 1 ]; do
#                 i=$(($i+1))
#                 temp_round=$(($temp_round+1))
#                 $VERBOSE && e_primary " running topic with active learning $temp_round"
#                 
#                 
#                 
#                 
#                 #cat -n $sub_sample new"$round".$TOPIC > sub_new"$round".$TOPIC
#                 step=$((i+1))
#                 begin=$((i*roundSize+1))
#                 end=$((i*roundSize+roundSize))
#                 
#                 sed -n "${begin},${end}p" new"$round".$TOPIC > sub_new_active_$temp_round.$TOPIC
# 
#                 echo "subsample relevants size `cat sub_new_active_$temp_round.$TOPIC | sort | join - goldendb.$TOPIC | wc -l `"
#                 
#                 if [ `wc -l < sub_new_active_$temp_round.$TOPIC` -lt 2 ]; then
#                     echo "continue processing"
#                     break
#                 fi
#                 
#                 acc=$(($rel_found_sample +$acc))
#                 source ../active_learning $CORPUS `echo $VERBOSE` $TOPIC "$temp_round" $discretize 2 sub_new_active_$temp_round.$TOPIC
#                 rel_found_sample=`wc -l < x_posit."$temp_round"`
#             done
#             
            $VERBOSE && e_success "Done."



        fi

        $VERBOSE && e_success "Active learning step finished."

        #-----------------------------------------------------------------
        # statics calculation
        #-----------------------------------------------------------------
        not_relevants=$(($not_relevants + `wc -l < x_negat."$round"`))

        
        
#         if [ $acc -eq 0 ]; then
#             memory=$(($memory +1))
#             if [ $memory -gt 5 ]; then
#                 echo "memory stop"
#                 break
#             fi
#         else
#             memory=0
#         fi
#        echo "memory value is $memory"
        rel_found_sample=`wc -l < x_posit."$round"`  # relevants on previous sample
        rel_rate=`echo "scale=4; $rel_found_sample / $SAMPLE_SIZE" | bc`
        current_rel=`cat x_posit.* | wc -l`

        # TODO: Check if all_docs may be replaced with TOTAL_DOCUMENTS.
        already_labeled_docs=`cat ssarp* | wc -l`
        all_docs=`cat new[0-9][0-9].$TOPIC | wc -l`

        
        $VERBOSE && e_info "\tNegative documents .............. `wc -l < x_negat.$round`"
            
           
        temp=$((`wc -l < sub_new_positives.$round`- `wc -l < x_posit.$round`))
        MISSING_REL=$(($temp+$MISSING_REL))    
        
#             
            
        $VERBOSE  && e_info "\tRelevants lab ..... $rel_found_sample"
        $VERBOSE  && e_info "\tNot Relevants lab. `wc -l < x_negat."$round"`"
        
        $VERBOSE  && e_info "\tSample size ....... $SAMPLE_SIZE"
        $VERBOSE  && e_info "\tRel in the Sample . $all_positive"
        $VERBOSE  && e_info "\tSub sample size ... $sub_sample"   
        
        $VERBOSE  && e_info "\tSub sample Rel Doc. `wc -l < sub_new_positives.$round`"
        $VERBOSE  && e_info "\tSample Rel Doc..... `cat new"$round".$TOPIC | sort | join - goldendb.$TOPIC | wc -l`" 
     
        $VERBOSE  && e_info "\tAlready labeled ... $already_labeled_docs"
        $VERBOSE  && e_info "\tAll docs .......... $all_docs"
        $VERBOSE  && e_info "\tRelevants rate .... $rel_rate"
        $VERBOSE  && e_info "\tCurrent relevants . $current_rel"
        $VERBOSE  && e_info "\tTotal relevants ... `wc -l < goldendb.$TOPIC`"
        $VERBOSE  && e_info "\tRevelants missed .. $MISSING_REL"

        relevants=$(($relevants + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo $rel_found_sample $SAMPLE_SIZE $sub_sample $rel_rate $current_rel >> rel.rate

        #sort goldendb.$TOPIC | sed -e 's/$/ 1/' > prel.$TOPIC
        #cut -d' ' -f1 prel.$TOPIC > goldendb.$TOPIC

        estimate=$(($estimate + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo "${round%.*} $estimate" >> store_estimation


        $VERBOSE && e_secondary "Estimate rate is: $estimate"


        LABELED_DOCUMENTS=$(($LABELED_DOCUMENTS + $SAMPLE_SIZE))
        #SAMPLE_SIZE=$(($SAMPLE_SIZE + ($SAMPLE_SIZE + 9) / 10))
        SAMPLE_SIZE=$(($SAMPLE_SIZE *2))
    fi
done

# TODO: Change this approach
estimate=$estimate
TOTAL_DOCUMENTS=$TOTAL_DOCUMENTS
rules=$rules
N=$round

popd
