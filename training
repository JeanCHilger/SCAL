#!/bin/bash

#===============================================================================
#       USAGE:  ./training <topic> <query> <judgeclass> <verbose>
#                          <keysize> <valsize>
#
# DESCRIPTION:  Contains the training loop that judges files as relevant or not.
#===============================================================================

source "${ABS_PATH}/handle_errors"
source "${ABS_PATH}/colors"

SOFIA="${ABS_PATH}/sofia-ml-read-only/sofia-ml"

TOPIC=$1; shift
QUERY=$1; shift
JUDGECLASS=$1; shift
VERBOSE=false; [[ $1 == "true" ]] && VERBOSE=true; shift
KEYSIZE=$1; shift
VALSIZE=$1; shift
ROUND=$1; shift

MAXTHREADS=5
rules=5

pushd $TOPIC


R=100                                   # Document sample size going to trainset
LAMBDA=0.0001                           # Lambda parameter for SVM
TOTAL_DOCUMENTS=`cat docfils | wc -l`   # Total quantity of documents
LABELED_DOCUMENTS=0                     # Total quantity of labeled documents
SAMPLE_SIZE=1                           # Number of documents to be labeled in the current round
TEMP_FILE=$(mktemp)                     # Temporary file for file redirections
sub_sample=0                            # Sub sample, limited to 30

MISSING_REL=0                           # Total number of relevant docs missed by active learning
discretize=1                            #if the dataset need to be discretized to run active learning
flag_allac_first_time=1
estimate=0                               #

not_relevants=`cat x_negat.* | wc -l `                          # Documents not relevant so far
relevants=`cat x_posit.* | wc -l `                              # Documents relevant so far
SAMPLE_SIZE=$(($not_relevants+$relevants))


function check_file {
    if [ `wc -l < $1` -eq 0 ]; then
            $VERBOSE && e_secondary "Error FILE $1 EQUAL TO ZERO"
            break;
    fi
}



#-------------------------------------------------------------------------
# performs the 100 looping rounds
#-------------------------------------------------------------------------
for _round in $(seq -f "%02g" $ROUND 99); do

    if [ $LABELED_DOCUMENTS -lt $TOTAL_DOCUMENTS ]; then
        export round=$_round

        $VERBOSE && e_primary "Preparing trainset from round $round wiht $relevants docs relevant "

        try
        (
            #cp $TOPIC.synthetic.seed trainset

            cut -f2 docfils \
                | shuf -n $R \
                | sort \
                | .././indexer "$JUDGECLASS".db $KEYSIZE $VALSIZE \
                | sed -e's/[^ ]*/-1/' >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing seed..."

        try
        (
            cat ssarp* > seed
            cat goldendb | shuf -n 50 > seed
            cat seed \
                | sort \
                | join - goldendb \
                | sed -e 's/^/1 /' \
                | sort \
                | uniq > intermediate_seed

            cat seed \
                | sort \
                | join -v1 - goldendb \
                | shuf -n 50000 \
                | sed -e 's/^/-1 /' \
                | sort \
                | uniq  >> intermediate_seed

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Adding content from seed to trainset..."

        try
        (
            cut -d' ' -f2 intermediate_seed \
                | .././indexer $JUDGECLASS.db $KEYSIZE $VALSIZE \
                | cut -d' ' -f2- \
                | paste -d' ' <(cut -d' ' -f1 intermediate_seed) - \
                | sort -n >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # calculate the prevalence of relevant documents on training set
        #-----------------------------------------------------------------
        REL_ON_TRAINSET=`grep -E "^1\b" trainset | wc -l`
        NOT_REL_ON_TRAINSET=`grep -E "^-1\b" trainset | wc -l`
        PREVALENCE_RATE=`echo "scale=4;
            $REL_ON_TRAINSET / ($REL_ON_TRAINSET + $NOT_REL_ON_TRAINSET)" \
            | bc`

        echo $REL_ON_TRAINSET $NOT_REL_ON_TRAINSET $PREVALENCE_RATE >> prevalence.rate

        $VERBOSE && e_primary "Executing training algorithm (sofia-ml)..."
       
        try
        (
            $SOFIA --learner_type logreg-pegasos --loop_type roc \
                --lambda $LAMBDA --iterations 2000000 \
                --training_file trainset --dimensionality 3300000 \
                --model_out svm_model &> classifier_output  

            sofia_status=$?

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # tests the previously trained model
        #-----------------------------------------------------------------
        if [[ $sofia_status -eq "0" ]]; then
            for test_file in svm.test.*; do
                while [ "$(jobs | grep 'Running' | wc -l)" -ge "$MAXTHREADS" ]; do
                    sleep 1
                done

                $VERBOSE && e_primary "Testing previously trained model..."

#                 try
#                 (
                    $SOFIA --test_file $test_file --dimensionality 3300000 \
                        --model_in svm_model --results_file pout.$test_file  &>> classifier_output
                             

#                 ) 2> $STD_ERROR_OUT
# 
#                 catch || {
#                     exit_on_error
#                 }

                $VERBOSE && e_success "Done."

            done

            wait

        #-----------------------------------------------------------------
        # create test files
        #-----------------------------------------------------------------
        else
            try
            (
                #rm -f pout.svm.test.*
                cut -f2 docfils \
                    | sort -R \
                    | cat -n \
                    | sort -k2 \
                    | sed -e 's/ */-/' > pout.svm.test.1

            ) 2> $STD_ERROR_OUT

            catch || {
                exit_on_error
            }
        fi
        check_file pout.svm.test.*
        $VERBOSE && e_primary "Preparing ranking... "

        try
        (
            cat new[0-9][0-9].$TOPIC  | sort | uniq >  LABELED_DOCS.$TOPIC
            cat x_posit.* x_negat.* | cut -d' ' -f2  >> LABELED_DOCS.$TOPIC
            cut -f1 pout.svm.test.* \
                | ../fixnum \
                | cat -n \
                | join -o2.2,1.2 -t$'\t' - docfils \
                | sort -k1 -n  > inlr.out."$round".$TOPIC

                      
            check_file inlr.out."$round".$TOPIC
            
            sort -n LABELED_DOCS.$TOPIC \
                | join  -v2 - inlr.out."$round".$TOPIC -2 1 \
                | shuf \
                | sort -k 2 -r -g -s  > ranking."$round".$TOPIC
                
            check_file ranking."$round".$TOPIC

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }
        
        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing new"$round".$TOPIC."

        try
        (
            cat ranking."$round".$TOPIC \
                | cut -d' ' -f1 > new"$round".$TOPIC

            #cat new[0-9][0-9] > intermediate_seed

            if [ ""$round"" != "99" ] ; then
                head -$SAMPLE_SIZE new"$round".$TOPIC > $TEMP_FILE
                mv $TEMP_FILE new"$round".$TOPIC
            fi

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SCAL subsampling..."

        if [ $SAMPLE_SIZE -le 30 ]; then
            sub_sample=$SAMPLE_SIZE
        else
            sub_sample=30
        fi

        shuf -n $sub_sample new"$round".$TOPIC > sub_new"$round".$TOPIC

        # TODO: Check the viability of adding the output
        # of this script to a variable instead in a file

        python2.7 ../doJudgementMain.py --topic=$TOPIC \
            --judgefile=../judgement/qrels.$JUDGECLASS.list \
            --input=sub_new"$round".$TOPIC \
            --output=rel.$TOPIC."$round".Judged.doc.list \
            --record=$TOPIC.record.list

        cat rel.$TOPIC."$round".Judged.doc.list >> rel.$TOPIC.fil
        sort rel.$TOPIC.fil | uniq > $TEMP_FILE
        mv $TEMP_FILE rel.$TOPIC.fil

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SSARP stage..."

        #-----------------------------------------------------------------
        # the active learning method needs at least 1 negative and
        # 1 positive document
        #-----------------------------------------------------------------
        if [ $not_relevants -lt 1 ] || [ $relevants -lt 1 ]; then
            $VERBOSE && e_secondary "\tStoring seed in ssarp file..."

            cp sub_new"$round".$TOPIC ssarp"$round".$TOPIC

            cat sub_new"$round".$TOPIC \
                | sort \
                | uniq \
                | join - goldendb \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > x_posit."$round"

            cat sub_new"$round".$TOPIC \
                | sort \
                | uniq \
                | join - goldendb -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > x_negat."$round"

            cat  x_negat.* \
                | sort -k2 \
                | join - $JUDGECLASS.svm.fil.svd -2 1 -1 2 > seed_ssarp_docs.$TOPIC

            cat  x_posit.* \
                | shuf -n 1 \
                | sort -k2 \
                | join - $JUDGECLASS.svm.fil.svd -2 1 -1 2 >> seed_ssarp_docs.$TOPIC

            cut -d ' ' -f2- seed_ssarp_docs.$TOPIC  > seed_ssarp_labels.$TOPIC

            cp x_posit."$round" sub_new_positives."$round"
            cp x_negat."$round" sub_new_negatives."$round"

            ssarp_relevants=`wc -l < x_posit."$round"`

        #-----------------------------------------------------------------
        #
        #-----------------------------------------------------------------
        else
            $VERBOSE && e_primary "Call SSARP with input size `wc -l < sub_new"$round".$TOPIC` sub sub_new"$round".$TOPIC "
            
            source ../active_learning $CORPUS `echo $VERBOSE` $TOPIC "$round" $discretize 2            
            discretize=1
            
            
            
            
            $VERBOSE && e_success "Done."

           # exit
#             $VERBOSE && e_secondary "\tCreating files..."
# 
#             # TODO: Check if the files temp_posit and temp_negat
#             # could be united into a single file.
# 
#             cat sub_new$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/1 /' > temp_posit.$round.$TOPIC
# 
#             cat sub_new$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb -v1 \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/-1 /' > temp_negat.$round.$TOPIC
# 
#             cat temp_posit.$round.$TOPIC temp_negat.$round.$TOPIC \
#                 | sort -k2 \
#                 | join - ../$JUDGECLASS.svm.fil.svd -2 1 -1 2 > trainset.$round.$TOPIC
# 
#             cut -d ' ' -f2- trainset.$round.$TOPIC  > trainset_labels.$round.$TOPIC
# 
#             #-------------------------------------------------------------
#             #
#             #-------------------------------------------------------------
#             if [ $discretize -eq 0 ]; then
#                 $VERBOSE && e_secondary "\tGenerating bins"
# 
#                 python3 ../svd/convert_txt.py \
#                     ../$JUDGECLASS.svm.fil.svd \
#                     /tmp/total.$TOPIC.arff goldendb
# 
#                 pushd ../SSARP/run/
#                 rm train*
#                 .././gera_bins_TUBE.sh /tmp/total.$TOPIC.arff 50 10 10
#                 popd
# 
#                 discretize=1
#             fi
# 
#             $VERBOSE && e_secondary "\tConverting files to txt..."
# 
#             python3 ../svd/convert_txt.py \
#                 trainset_labels.$round.$TOPIC \
#                 out.$round.$TOPIC.arff \
#                 goldendb
# 
#             python3 ../svd/convert_txt.py \
#                 seed_ssarp_labels.$TOPIC \
#                 seed_out.$round.$TOPIC.arff \
#                 goldendb
# 
#             cp trainset.$round.$TOPIC ../SSARP/run/
#             cp seed_out.$round.$TOPIC.arff ../SSARP/run/
#             cp out.$round.$TOPIC.arff ../SSARP/run/
# 
#             $VERBOSE && e_primary "\tExecuting alac rules..."
# 
#             pushd ../SSARP/run/
#             ./SSARPX.sh out.$round.$TOPIC \
#                 trainset.$round.$TOPIC \
#                 50 \
#                 $round \
#                 seed_out.$round.$TOPIC.arff \
#                 $TOPIC \
#                 $flag_allac_first_time \
#                 $rules
# 
#             flag_allac_first_time=$(($flag_allac_first_time+1))
#             cat label.$round.$TOPIC > /tmp/ssarp$round.$TOPIC
# 
#             popd
# 
#             $VERBOSE && e_success "\tDone."
# 
#             mv /tmp/ssarp$round.$TOPIC .
# 
#             cat sub_new$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/1 /' > sub_new_positives.$round
# 
#             cat sub_new$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb -v1 \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/-1 /' > sub_new_negatives.$round
# 
#             # TODO: Check if the files x_posit and x_negat
#             # could be united into a single file.
# 
#             cat ssarp$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/1 /' > x_posit.$round
# 
#             cat ssarp$round.$TOPIC \
#                 | sort \
#                 | uniq \
#                 | join - goldendb -v1 \
#                 | cut -d' ' -f1 \
#                 | sed -e 's/^/-1 /' > x_negat.$round
# 
#             cat x_posit.$round x_negat.$round \
#                 | cut -d' ' -f2 \
#                 | sort \
#                 | uniq > ssarp$round.$TOPIC
# 
#             ssarp_relevants=`wc -l < x_posit.$round`
# 
#             $VERBOSE && e_info \
#                 "Positives documents collected ... `wc -l < x_posit.$round`"
#             $VERBOSE && e_info \
#                 "Negative documents .............. `wc -l < x_negat.$round`"
#             $VERBOSE && e_info \
#                 "Total positives in sub sample ... `wc -l < sub_new_positives.$round`"
# 
#             echo "Positives documents collected ... `wc -l < x_posit.$round`\
#             Negative documents .............. `wc -l < x_negat.$round`\
#             Total positives in sub sample ... `wc -l < sub_new_positives.$round`" >> runs.log

        fi

        $VERBOSE && e_success "Active learning step finished."

        #-----------------------------------------------------------------
        # statics calculation
        #-----------------------------------------------------------------
        not_relevants=$(($not_relevants + `wc -l < x_negat."$round"`))

        rel_found_sample=`wc -l < x_posit."$round"`  # relevants on previous sample
        rel_rate=`echo "scale=4; $rel_found_sample / $SAMPLE_SIZE" | bc`
        current_rel=`cat x_posit.[0-9][0-9] | wc -l`

        # TODO: Check if all_docs may be replaced with TOTAL_DOCUMENTS.
        already_labeled_docs=`cat ssarp* | wc -l`
        all_docs=`cat new[0-9][0-9].$TOPIC | wc -l`

        
#             $VERBOSE && e_info "\t\tNegative documents .............. `wc -l < x_negat.$round`"
#             
#            
#             temp=$((`wc -l < sub_new_positives.$round`- `wc -l < x_posit.$round`))
#             MISSING_REL=$(($temp+$MISSING_REL))
#             $VERBOSE && e_info       "\t\tRevelants missed ... $MISSING_REL"
#             
            
        $VERBOSE && e_info "\tRelevants ......... $rel_found_sample"
        $VERBOSE && e_info "\tSample size ....... $SAMPLE_SIZE"
        $VERBOSE && e_info "\tSub sample size ... $sub_sample"   
        $VERBOSE && e_info "\tTotal positives in the sub sample ... `wc -l < sub_new_positives.$round`"
        $VERBOSE && e_info "\tTotal positives in sample ... `cat new"$round".$TOPIC | sort | join - goldendb | wc -l`" 
        $VERBOSE && e_info "\tAlready labeled ... $already_labeled_docs"
        $VERBOSE && e_info "\tAll docs .......... $all_docs"
        $VERBOSE && e_info "\tRelevants rate .... $rel_rate"
        $VERBOSE && e_info "\tCurrent relevants . $current_rel"
        $VERBOSE && e_info "\tTotal relevants ... `wc -l < goldendb`"

        relevants=$(($relevants + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo $rel_found_sample $SAMPLE_SIZE $sub_sample $rel_rate $current_rel >> rel.rate

        sort goldendb | sed -e 's/$/ 1/' > prel.$TOPIC
        cut -d' ' -f1 prel.$TOPIC > goldendb

        estimate=$(($estimate + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo "${round%.*} $estimate" >> store_estimation


        $VERBOSE && e_secondary "Estimate rate is: $estimate"


        LABELED_DOCUMENTS=$(($LABELED_DOCUMENTS + $SAMPLE_SIZE))
        SAMPLE_SIZE=$(($SAMPLE_SIZE + ($SAMPLE_SIZE + 9) / 10))
    fi
done

# TODO: Change this approach
export estimate=$estimate
export TOTAL_DOCUMENTS=$TOTAL_DOCUMENTS
export rules=$rules
export N=$round

popd
