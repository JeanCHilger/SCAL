#!/bin/bash

#===============================================================================
#       USAGE:  ./training <topic> <query> <judgeclass> <verbose>
#                          <keysize> <valsize>
#
# DESCRIPTION:  Contains the training loop that judges files as relevant or not.
#===============================================================================

source "${ABS_PATH}/handle_errors"
source "${ABS_PATH}/colors"

SOFIA="${ABS_PATH}/sofia-ml-read-only/sofia-ml"

TOPIC=$1; shift
QUERY=$1; shift
JUDGECLASS=$1; shift
VERBOSE=false; [[ $1 == "true" ]] && VERBOSE=true; shift
KEYSIZE=$1; shift
VALSIZE=$1; shift

MAXTHREADS=5
rules=5

pushd $TOPIC


R=100                                   # Document sample size going to trainset
LAMBDA=0.0001                           # Lambda parameter for SVM
TOTAL_DOCUMENTS=`cat docfils | wc -l`   # Total quantity of documents
LABELED_DOCUMENTS=0                     # Total quantity of labeled documents
SAMPLE_SIZE=1                           # Number of documents to be labeled in the current round
TEMP_FILE=$(mktemp)                     # Temporary file for file redirections
sub_sample=0                            # Sub sample, limited to 30

discretize=0
flag_allac_first_time=0
estimate=0                               #

not_relevants=0                          # Documents not relevant so far
relevants=0                              # Documents relevant so far

#-------------------------------------------------------------------------
# performs the 100 looping rounds
#-------------------------------------------------------------------------
for _round in $(seq -f "%02g" 0 99); do

    if [ $LABELED_DOCUMENTS -lt $TOTAL_DOCUMENTS ]; then
        export round=$_round

        $VERBOSE && e_primary "Preparing trainset..."

        try 55 61
        (
            cp $TOPIC.synthetic.seed trainset

            cut -f2 docfils \
                | shuf -n $R \
                | sort \
                | .././indexer "$JUDGECLASS".db $KEYSIZE $VALSIZE \
                | sed -e's/[^ ]*/-1/' >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing seed..."

        try 75 91
        (
            [[ -e ssarp* ]] && \
                cat ssarp* > seed

            cat seed \
                | sort \
                | join - rel.$TOPIC.fil \
                | sed -e 's/^/1 /' \
                | sort \
                | uniq > intermediate_seed

            cat seed \
                | sort \
                | join -v1 - rel.$TOPIC.fil \
                | shuf -n 50000 \
                | sed -e 's/^/-1 /' \
                | sort \
                | uniq  >> intermediate_seed

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Adding content from seed to trainset..."

        try 105
        (
            cut -d' ' -f2 intermediate_seed \
                | .././indexer $JUDGECLASS.db $KEYSIZE $VALSIZE \
                | cut -d' ' -f2- \
                | paste -d' ' <(cut -d' ' -f1 intermediate_seed) - \
                | sort -n >> trainset

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # calculate the prevalence of relevant documents on training set
        #-----------------------------------------------------------------
        REL_ON_TRAINSET=`grep -E "^1\b" trainset | wc -l`
        NOT_REL_ON_TRAINSET=`grep -E "^-1\b" trainset | wc -l`
        PREVALENCE_RATE=`echo "scale=4;
            $REL_ON_TRAINSET / ($REL_ON_TRAINSET + $NOT_REL_ON_TRAINSET)" \
            | bc`

        echo $REL_ON_TRAINSET $NOT_REL_ON_TRAINSET $PREVALENCE_RATE >> prevalence.rate

        $VERBOSE && e_primary "Executing training algorithm (sofia-ml)..."

        try 134 139
        (
            $SOFIA --learner_type logreg-pegasos --loop_type roc \
                --lambda $LAMBDA --iterations 2000000 \
                --training_file trainset --dimensionality 3300000 \
                --model_out svm_model &> classifier_output

            sofia_status=$?

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        #-----------------------------------------------------------------
        # tests the previously trained model
        #-----------------------------------------------------------------
        if [[ $sofia_status -eq "0" ]]; then
            for test_file in svm.test.*; do
                while [ "$(jobs | grep 'Running' | wc -l)" -ge "$MAXTHREADS" ]; do
                    sleep 1
                done

                $VERBOSE && e_primary "Testing previously trained model..."

                try
                (
                    $SOFIA --test_file $test_file --dimensionality 3300000 \
                        --model_in svm_model --results_file pout.$test_file \
                        &> classifier_output &

                ) 2> $STD_ERROR_OUT

                catch || {
                    exit_on_error
                }

                $VERBOSE && e_success "Done."

            done

            wait

        #-----------------------------------------------------------------
        # create test files
        #-----------------------------------------------------------------
        else
            try
            (
                rm -f pout.svm.test.*
                cut -f2 docfils \
                    | sort -R \
                    | cat -n \
                    | sort -k2 \
                    | sed -e 's/ */-/' > pout.svm.test.1

            ) 2> $STD_ERROR_OUT

            catch || {
                exit_on_error
            }
        fi

        $VERBOSE && e_primary "Preparing ranking.$round.$TOPIC..."

        try
        (
            cat new[0-9][0-9].$TOPIC > seed.$TOPIC

            cut -f1 pout.svm.test.* \
                | ../fixnum \
                | cat -n \
                | join -o2.2,1.2 -t$'\t' - docfils \
                | sort -k1 -n  > inlr.out.$round.$TOPIC

# CHANGE 1: sort -n seed.topic > temp; cat temp |...
            sort -n seed.$TOPIC \
                | join  -v2 - inlr.out.$round.$TOPIC -2 1 \
                | shuf \
                | sort -k 2 -r -g -s  > ranking.$round.$TOPIC

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Preparing new$round.$TOPIC."

        try
        (
            cat ranking.$round.$TOPIC \
                | cut -d' ' -f1 > new$round.$TOPIC

            cat new[0-9][0-9] > intermediate_seed

            if [ "$round" != "99" ] ; then
                head -$SAMPLE_SIZE new$round.$TOPIC > $TEMP_FILE
                mv $TEMP_FILE new$round.$TOPIC
            fi

        ) 2> $STD_ERROR_OUT

        catch || {
            exit_on_error
        }

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SCAL subsampling..."

        if [ $SAMPLE_SIZE -le 30 ]; then
            sub_sample=$SAMPLE_SIZE
        else
            sub_sample=30
        fi

        shuf -n $sub_sample new$round.$TOPIC > sub_new$round.$TOPIC

        # TODO: Check the viability of adding the output
        # of this script to a variable instead in a file

        python2.7 ../doJudgementMain.py --topic=$TOPIC \
            --judgefile=../judgement/qrels.$JUDGECLASS.list \
            --input=sub_new$round.$TOPIC \
            --output=rel.$TOPIC.$round.Judged.doc.list \
            --record=$TOPIC.record.list

        cat rel.$TOPIC.$round.Judged.doc.list >> rel.$TOPIC.fil
        sort rel.$TOPIC.fil | uniq > $TEMP_FILE
        mv $TEMP_FILE rel.$TOPIC.fil

        $VERBOSE && e_success "Done."

        $VERBOSE && e_primary "Starting SSARP stage..."

        #-----------------------------------------------------------------
        # the active learning method needs at least 1 negative and
        # 1 positive document
        #-----------------------------------------------------------------
        if [ $not_relevants -lt 1 ] || [ $relevants -lt 1 ]; then
            $VERBOSE && e_secondary "\tStoring seed in ssarp file..."

            cp sub_new$round.$TOPIC ssarp$round.$TOPIC

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > x_posit.$round

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > x_negat.$round

            cat  x_negat.* \
                | sort -k2 \
                | join - ../$JUDGECLASS.svm.fil.svd -2 1 -1 2 > seed_ssarp_docs.$TOPIC

            cat  x_posit.* \
                | shuf -n 1 \
                | sort -k2 \
                | join - ../$JUDGECLASS.svm.fil.svd -2 1 -1 2 >> seed_ssarp_docs.$TOPIC

            cut -d ' ' -f2- seed_ssarp_docs.$TOPIC  > seed_ssarp_labels.$TOPIC

            cp x_posit.$round sub_new_positives.$round
            cp x_negat.$round sub_new_negatives.$round

            ssarp_relevants=`wc -l < x_posit.$round`

        #-----------------------------------------------------------------
        #
        #-----------------------------------------------------------------
        else

            $VERBOSE && e_secondary "\tCreating files..."

            # TODO: Check if the files temp_posit and temp_negat
            # could be united into a single file.

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > temp_posit.$round.$TOPIC

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > temp_negat.$round.$TOPIC

            cat temp_posit.$round.$TOPIC temp_negat.$round.$TOPIC \
                | sort -k2 \
                | join - ../$JUDGECLASS.svm.fil.svd -2 1 -1 2 > trainset.$round.$TOPIC

            cut -d ' ' -f2- trainset.$round.$TOPIC  > trainset_labels.$round.$TOPIC

            #-------------------------------------------------------------
            #
            #-------------------------------------------------------------
            if [ $discretize -eq 0 ]; then
                $VERBOSE && e_secondary "\tGenerating bins"

                python3 ../svd/convert_txt.py \
                    ../$JUDGECLASS.svm.fil.svd \
                    /tmp/total.$TOPIC.arff rel.$TOPIC.fil

                pushd ../SSARP/run/
                rm train*
                .././gera_bins_TUBE.sh /tmp/total.$TOPIC.arff 50 10 10
                popd

                discretize=1
            fi

            $VERBOSE && e_secondary "\tConverting files to txt..."

            python3 ../svd/convert_txt.py \
                trainset_labels.$round.$TOPIC \
                out.$round.$TOPIC.arff \
                rel.$TOPIC.fil

            python3 ../svd/convert_txt.py \
                seed_ssarp_labels.$TOPIC \
                seed_out.$round.$TOPIC.arff \
                rel.$TOPIC.fil

            cp trainset.$round.$TOPIC ../SSARP/run/
            cp seed_out.$round.$TOPIC.arff ../SSARP/run/
            cp out.$round.$TOPIC.arff ../SSARP/run/

            $VERBOSE && e_primary "\tExecuting alac rules..."

            pushd ../SSARP/run/
            ./SSARPX.sh out.$round.$TOPIC \
                trainset.$round.$TOPIC \
                50 \
                $round \
                seed_out.$round.$TOPIC.arff \
                $TOPIC \
                $flag_allac_first_time \
                $rules

            flag_allac_first_time=$(($flag_allac_first_time+1))
            cat label.$round.$TOPIC > /tmp/ssarp$round.$TOPIC

            popd

            $VERBOSE && e_success "\tDone."

            mv /tmp/ssarp$round.$TOPIC .

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > sub_new_positives.$round

            cat sub_new$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > sub_new_negatives.$round

            # TODO: Check if the files x_posit and x_negat
            # could be united into a single file.

            cat ssarp$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil \
                | cut -d' ' -f1 \
                | sed -e 's/^/1 /' > x_posit.$round

            cat ssarp$round.$TOPIC \
                | sort \
                | uniq \
                | join - rel.$TOPIC.fil -v1 \
                | cut -d' ' -f1 \
                | sed -e 's/^/-1 /' > x_negat.$round

            cat x_posit.$round x_negat.$round \
                | cut -d' ' -f2 \
                | sort \
                | uniq > ssarp$round.$TOPIC

            ssarp_relevants=`wc -l < x_posit.$round`

            $VERBOSE && e_info \
                "Positives documents collected ... `wc -l < x_posit.$round`"
            $VERBOSE && e_info \
                "Negative documents .............. `wc -l < x_negat.$round`"
            $VERBOSE && e_info \
                "Total positives in sub sample ... `wc -l < sub_new_positives.$round`"

            echo "Positives documents collected ... `wc -l < x_posit.$round`\
            Negative documents .............. `wc -l < x_negat.$round`\
            Total positives in sub sample ... `wc -l < sub_new_positives.$round`" >> runs.log

        fi

        $VERBOSE && e_success "Active learning step finished."

        #-----------------------------------------------------------------
        # statics calculation
        #-----------------------------------------------------------------
        not_relevants=$(($not_relevants + `wc -l < x_negat.$round`))

        rel_found_sample=`wc -l < x_posit.$round`  # relevants on previous sample
        rel_rate=`echo "scale=4; $rel_found_sample / $SAMPLE_SIZE" | bc`
        current_rel=`cat x_posit.[0-9][0-9] | wc -l`

        # TODO: Check if all_docs may be replaced with TOTAL_DOCUMENTS.
        already_labeled_docs=`cat ssarp* | wc -l`
        all_docs=`cat new[0-9][0-9].$TOPIC | wc -l`

        $VERBOSE && e_info "\tRelevants ......... $rel_found_sample"
        $VERBOSE && e_info "\tSample size ....... $SAMPLE_SIZE"
        $VERBOSE && e_info "\tSub sample size ... $sub_sample"
        $VERBOSE && e_info "\tAlready labeled ... $already_labeled_docs"
        $VERBOSE && e_info "\tAll docs .......... $all_docs"
        $VERBOSE && e_info "\tRelevants rate .... $rel_rate"
        $VERBOSE && e_info "\tCurrent relevants . $current_rel"
        $VERBOSE && e_info "\tTotal relevants ... `wc -l < rel.$TOPIC.fil`"

        relevants=$(($relevants + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo $rel_found_sample $SAMPLE_SIZE $sub_sample $rel_rate $current_rel >> rel.rate

        sort rel.$TOPIC.fil | sed -e 's/$/ 1/' > prel.$TOPIC
        cut -d' ' -f1 prel.$TOPIC > rel.$TOPIC.fil

        estimate=$(($estimate + \
            ($rel_found_sample * $SAMPLE_SIZE) / $sub_sample))

        echo "${round%.*} $estimate" >> store_estimation


        $VERBOSE && e_secondary "Estimate rate is: $estimate"


        LABELED_DOCUMENTS=$(($LABELED_DOCUMENTS + $SAMPLE_SIZE))
        SAMPLE_SIZE=$(($SAMPLE_SIZE + ($SAMPLE_SIZE + 9) / 10))
    fi
done

# TODO: Change this approach
export estimate=$estimate
export TOTAL_DOCUMENTS=$TOTAL_DOCUMENTS
export rules=$rules
export N=$round

popd
